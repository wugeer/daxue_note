组合分类器是一个复合模型，有多个分类器组合而成。个体分类器投票，组合分类器基于投票返回类标号预测。
组合分类方法简介:
    装袋，随机森林，提升都是组合方法的例子。
    组合分类把k个学习得到的模型（或基分类器）M1，M2，。。。，Mk组合在一起。
    使用给定的数据集D创建k个训练集，每个训练集用于训练一个模型。对一个待分类的新数据元组，每个基分类器返回类预测投票。组合分类器基于基分类器投票返回类预测。
    当模型之间存在显著差异时，组合分类器产生更好的结果。理想地，基分类器之间几乎不相关，基分类器还应优于随机猜测
    
装袋：
    规定d个元组的集合D，装袋过程如下：对于迭代i（i为一到k之间的正整数），d个元组的训练集Di采用有放回抽样，由原始数据集D抽取。术语装袋表示自助聚集，每个训练集都是一个自助样本。得到所有的装袋分类器后，每个分类器对待分类的未知元组进行投票，得票最多的类赋予为未知元组。
   
提升和AdaBoost
    在提升方法中，权重赋予每个训练元组，迭代地学习k个分类器，学习得到分类器Mi后，更新权重，使得其后的分类器Mi+1更关注Mi误分类的训练元组。最终提升的分类器组合每个个体分类器的表决，其中每个分类器投票的权重是其准确率的函数。
    过程：开始时，AdaBoost对每个训练元组赋予相等的权重1/d，为组合分类器产生k个基分类器需要执行算法的其余部分k轮，在第i轮，从D中元组抽样形成大小为d的训练集Di。使用有放回的抽样，同一个元组可能被多次选中，每个元组被选中的机会由它的权重决定。从训练集Di导出分类器Mi，然后使用Di作为检验集计算Mi的误差，训练元组的权重根据它们的分类情况调整，如果元组被不正确分类，则它的权重增加，反之，减少。权重越高，越可能错误地分类。然后使用这些权重，为下一轮的分类器产生训练样本。基本思想是当建立分类器时，希望他更关注于上一轮被误分类的元组。
算法：
    1 将D中每个元组的权重初始化为1/d
    2 for i=1 to k do
    3    根据元组的权重从D中有放回的抽样，得到Di
    4    Di导出模型Mi
    5    错误率error(Mi)(ps:下面会介绍这个计算方法）
    6    if error(Mｉ）> 0.5 then 
    7         转３重试
    ８　　ｅｎｄif
    9    for Di的每个被正确分类的元组　do
    10       元组的权重乘以error(Mi)/(1-error(Mi));//更新权重
    １１　规范化每个元组的权重
    １２　endfor
使用组合分类器对元组x分类：
    将每个类的权重初始化为０
    for i=1 to k do   　　　　　　　　　　//对于每个分类器
       wi=log((1-error(Mi))/error(Mi))　//分类器的投票权重
       c = Mi(x)　　　　　　　　　　　　　　//从Ｍｉ得到x的类预测
       将ｗｉ加到类i的权重
    endfor
    返回具有最大权重的类
    
error(Mi)=wi*err(Xj)的求和,其中err(Xj)是元组Ｘｊ的误分类误差；如果Ｘｊ被误分类，那么值为１，否则为０。
由于关注误分类，存在过拟合的情况。

随机森林
    随机森林可以用装袋和随机属性选择结合来构建。
    给定ｄ个元组的训练集Ｄ，为组合分类器产生k棵决策树的一般过程如下：对于每次迭代ｉ，使用有放回抽样，由Ｄ产生ｄ个元祖的训练集Ｄｉ。设Ｆ是用来在每个节点随机选择Ｆ个属性作为该节点划分的候选属性。使用CART算法的方法来增长树。树增长到最大规模，并且不剪枝。使用随机输入选择形成的随机森林称为Forest-RI
    另一种形式为Forest-RC，使用输入属性的随机线性组合。由已有的属性的线性组合创建一些新的属性（特征）。在每个给定的节点，随机选择Ｌ个属性，并且随机从[-1,1]中随机选择一个数作为系数想家。产生Ｆ个线性组合，在其中搜索最佳划分。
    
提高类不平衡数据的分类准确率
    方法：
        过抽样：对正元组重复采样，使得结果训练集中包含相同个数的正负元组
        欠抽样：减少负元组的数量。它随机从多数（负）类中删除元组，知道正元组和负元组个数相等。
        阈值移动：不平衡类的阈值移动方法不涉及抽样。他用于对给定输入元组返回一个连续输出值的分类器。即对于给定元组Ｘ，这种分类器返回一个映射ｆ(X),输出值在０和１之间。对于输出值大于某一个阈值的元组视为正的。一般而言，阈值移动方法移动阈值t，使得稀有类的元组容易分类。
    