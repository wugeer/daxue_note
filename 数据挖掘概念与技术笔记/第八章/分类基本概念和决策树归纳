预测问题的两种主要类型：分类和数值预测
分类的一般方法：
    数据分类的一般过程：学习阶段（构建分类模型），分类阶段（使用模型预测给定数据的类标号）
    第一阶段：建立描述预先定义的数据类或概念集的分类器。训练集由数据库元组和他们相关联的类标号组成。类标号属性是离散的和无序的。或者把这个阶段看做学习一个映射或函数，它可以预测给定元祖的类标号。
    在第二阶段，使用模型进行分类。首先就是评估分类器/模型的预测准确率。避免过拟合和拟合程度过低等不好情况。准确率是分类器正确分类的检验元组所占百分比。
    
决策树归纳：
    从有类标号的训练元组中学习决策树。决策树中每个内部节点（非叶节点）表示在一个属性上的测试，每个分支代表一个输出，叶子节点存放类标号。决策树分类器的构造不需要任何领域知识或参数设置，适合探测式知识发现。可以处理高维数据。
    ID3,C4.5,CART都采用贪心算法（非回溯的）方法，决策树以自顶向下递归的分治方式构造。
    决策树生成算法：
    输入：
        数据分区Ｄ，训练元组和他们对应的类标号集合
        attribute_list候选属性的集合
        Attribute_selection_method一个确定最好地划分数据元组为个体类的分裂准则的过程。这个准则由分类属性和分裂点或划分子集组成。
    输出：决策树
    方法：
        １创建一个节点Ｎ
        ２if Ｄ中的元组都在同一个类C中 then 
        3     返回Ｎ作为叶节点，标记为类C
        ４if attribute_list为空（没有属性可以选了）　then 
        5     返回Ｎ作为叶节点，标记为Ｄ中的大多数类　　　//多数人说的算 
        ６使用Attribute_selection_method(D,attribute_list),找到最好的splitting_criterion;
        7用splitting_criterion标记节点Ｎ
        ８if splittinf_attribute是离散值的，并且允许多路划分　then 
        9    attribute_list删除分裂属性splitting_attribute
        10for splitting_criterion中的每个输出j  //划分元组并对每个分区产生子树
        11   设Ｄj是Ｄ中满足输出j的数据元组的集合　　//分区
        １２　if Dj为空　then 
        13       加一个树叶到节点Ｎ，标记为Ｄ中的多数类
        １４　else
        15       加一个由Generate_decision_tree(Dj,attribute_list)返回的节点到Ｎ
        １６endfor
        17返回Ｎ
    描述：
        首先从根节点开始，每次从候选属性集中找到最好的分裂属性并删除之，由此找到分裂属性的值，作为下一个分支，如果分类属性值的数据元组为空，那么该节点为叶子节点
    考虑：
        属性值是离散的，对Ａ中每个已知值创建一个分枝，并用该值标记；属性值是连续的，确定分裂点，输出大于分裂值和不大于分裂值两部分数据集作为节点分支。
        属性值是离散的，且必须产生二叉树：将属性值所有值分为两组
        迭代终止条件：
        １分区Ｄ（在节点Ｎ提供）的所有元组都属于同一个类
        ２没有剩余属性可以用来进一步划分元组，此时采用多数表决
        ３给定分支没有分组
属性选择度量：
    １信息增益
        选择有最高信息增益的的属性作为节点Ｎ的分裂属性，使得结果分区中对元组分类所需要的信息量最小，并反映这些分区中的最小随机性或不存性
        识别Ｄ中元组的类标号所需要的平均信息量Info(D)=-(每个类的概率乘以以２为底值为概率的对数的值的累加和),每个类的概率用每个类的个数除以总个数确定
        信息增益Gain(A)=在Ｄ数据集上划分需要的平均信息量减去按Ａ划分后每个子分区的所需要的信息量，后者的计算为：每个子分区的所占比例乘以每个子分区确定类标号所需要的平均信息量，在求和即得到
        分裂点不一定需要是分类属性已有的值，可以是两个已有值的平均值。
        偏向于多值属性
    ２增益率
        SplitInfo_A(D)=-(每个分区的比例乘以以２为底值为比例的对数值的累加和)
        GrianRate(A)=Grain(A)/SplitInfo_A(D)
        调整信息增益的偏倚
    3基尼系数
        Gini(D)=1-每个类的概率的平方的和
        Gini_A(D)=Ｄ中每个分区的比例乘以该分区的基尼系数的累加和

树剪枝：
    先剪枝：通过提前停止树的构建而对树剪枝。一旦停止，节点变为树叶。该树叶可以持有子集元组中最频繁的类或者这些元组的概率分布
    后剪枝：由完全生长的树减去子树
    CART使用代价复杂度算法是后剪枝方法的一个实例。把树的复杂度看做树中树叶节点的个数和树的错误率的函数（错误率是树误分类元组的百分比）。从树的底部开始，对于每个内部节点Ｎ，计算Ｎ的子树代价复杂度和该子树剪枝后Ｎ的子树（用一个树叶节点代替）的代价复杂度。比较这两个值，如果剪去节点Ｎ的子树导致较小的代价复杂度，则剪去该子树。否则，保留。使用一个标记类元组的剪枝集来评估代价复杂度，该集合独立于用于建立未剪枝树的训练集和用于准确率评估的检验集。算法产生一个渐进的剪枝树的集合。一般而言，最小化代价复杂度的最小决策树是首选。
    Ｃ4.5使用一种称为悲观剪枝的方法，也使用错误率评估，对子树剪枝做出决定，不需要使用剪枝集，而是使用训练集估计错误率。这种方法过于乐观，需要加上一个惩罚来抵消出现的偏倚。
    可以根据树编码所需要的二进位位数，而不是根据错误率。最佳剪枝树是最小化编码二进位位数的树。采用MDL,基本思想是:最简单的解是首选。
    
可伸缩性和决策树归纳
    可以处理可伸缩问题的决策树算法：
        雨林算法能适应可用的内存量，并用于任意决策树归纳算法。该方法在每个节点，对每个属性维护一个AVC-集，其中AVC表示“属性－值，类标号”，描述该节点的训练元组。节点Ｎ上属性Ａ的AVC集给出Ｎ上元组Ａ的每个值的类标号计数。节点Ｎ上所有AVC集的集合是Ｎ的AVC组群；
        树构造的自助乐观算法BOAT,采用完全不同的可伸缩算法－它不基于特殊数据结构的使用，而是使用一种称为自助法的技术。创建给定训练数据的一些较小的样本（或子集），其中每个子集都能放在内存中。使用每个子集构造一棵树，导致多棵树。考察这些树并使用他们构造一颗新树，它非常接近与原来的所有训练数据都放在内存所产生的树。
        通常ＢＯＡＴ只需要扫描Ｄ两次。可以增量更新

决策树归纳的可视化挖掘：
    基于感知的分类(PBC)是一种基于多维可视化技术决策树归纳方法，允许用户在构建决策树时加上关于数据的背景知识。
    ＰＢＣ使用一种基于像素的方法观察具有类标号信息的多维数据，采用扇形方法把多维数据对象映射到一个被划分为ｄ个扇区的圆，其中每个扇形代表一个属性。这里数据对象的一个属性值被映射到一个着色的像素，代表该对象的类标号。
    
        